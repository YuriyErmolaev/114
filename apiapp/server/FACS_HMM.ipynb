{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T07:54:08.006136Z",
     "start_time": "2025-10-01T07:54:07.339854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, site, platform\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Python ver:\", platform.python_version())\n",
    "!{sys.executable} -m pip --version\n",
    "!{sys.executable} -m pip show py-feat hmmlearn tqdm graphviz celluloid\n"
   ],
   "id": "5d9ac8b8ba03bc95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/bin/python\n",
      "Python ver: 3.11.13\n",
      "pip 24.3.1 from /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n",
      "Name: py-feat\r\n",
      "Version: 0.6.2\r\n",
      "Summary: Facial Expression Analysis Toolbox\r\n",
      "Home-page: https://github.com/cosanlab/py-feat\r\n",
      "Author: Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Eshin Jolly, Luke Chang\r\n",
      "Author-email: jcheong0428@gmail.com, eshin.jolly@gmail.com, luke.j.chang@dartmouth.edu\r\n",
      "License: MIT license\r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: av, celluloid, easing-functions, h5py, joblib, kornia, matplotlib, nltools, numexpr, numpy, pandas, Pillow, pywavelets, scikit-image, scikit-learn, seaborn, torchvision, tqdm, xgboost\r\n",
      "Required-by: \r\n",
      "---\r\n",
      "Name: hmmlearn\r\n",
      "Version: 0.3.3\r\n",
      "Summary: Hidden Markov Models in Python with scikit-learn like API\r\n",
      "Home-page: \r\n",
      "Author: David Cournapeau, Fabian Pedregosa, Gael Varoquaux, Sergei Lebedev, Antony Lee, Matthew Danielson\r\n",
      "Author-email: \r\n",
      "License: \r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: numpy, scikit-learn, scipy\r\n",
      "Required-by: \r\n",
      "---\r\n",
      "Name: tqdm\r\n",
      "Version: 4.67.1\r\n",
      "Summary: Fast, Extensible Progress Meter\r\n",
      "Home-page: https://tqdm.github.io\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MPL-2.0 AND MIT\r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: py-feat\r\n",
      "---\r\n",
      "Name: graphviz\r\n",
      "Version: 0.21\r\n",
      "Summary: Simple Python interface for Graphviz\r\n",
      "Home-page: https://github.com/xflr6/graphviz\r\n",
      "Author: \r\n",
      "Author-email: Sebastian Bank <sebastian.bank@uni-leipzig.de>\r\n",
      "License: \r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: \r\n",
      "---\r\n",
      "Name: celluloid\r\n",
      "Version: 0.2.0\r\n",
      "Summary: Easy matplotlib animation.\r\n",
      "Home-page: https://github.com/jwkvam/celluloid\r\n",
      "Author: Jacques Kvam\r\n",
      "Author-email: jwkvam+pypi@gmail.com\r\n",
      "License: UNKNOWN\r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: matplotlib\r\n",
      "Required-by: py-feat\r\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T07:54:11.778288Z",
     "start_time": "2025-10-01T07:54:10.029244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install -U py-feat hmmlearn tqdm graphviz celluloid\n",
    "# quick check and SciPy compatibility shim for py-feat expecting scipy.integrate.simps\n",
    "import scipy\n",
    "try:\n",
    "    from scipy import integrate as _integrate\n",
    "    if not hasattr(_integrate, \"simps\"):\n",
    "        from scipy.integrate import simpson as _simpson\n",
    "        setattr(_integrate, \"simps\", _simpson)\n",
    "except Exception as _e:\n",
    "    print(\"Warning: could not alias scipy.integrate.simpson to simps:\", _e)\n",
    "# Compatibility shim for environments where lib2to3 is unavailable (e.g., Py3.12+)\n",
    "try:\n",
    "    import sys as _sys, types as _types\n",
    "    if 'lib2to3.pytree' not in _sys.modules:\n",
    "        _lib2to3 = _types.ModuleType('lib2to3')\n",
    "        _pytree = _types.ModuleType('lib2to3.pytree')\n",
    "        def convert(node):  # minimal stub used by py-feat's ResMaskNet wrapper\n",
    "            return node\n",
    "        _pytree.convert = convert\n",
    "        _lib2to3.pytree = _pytree\n",
    "        _sys.modules['lib2to3'] = _lib2to3\n",
    "        _sys.modules['lib2to3.pytree'] = _pytree\n",
    "except Exception as _e:\n",
    "    print(\"Warning: could not create lib2to3 compatibility shim:\", _e)\n",
    "import feat, hmmlearn, tqdm, graphviz, celluloid\n",
    "print(\"feat:\", getattr(feat, \"__version__\", \"n/a\"))\n"
   ],
   "id": "79382fc2766f622c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-feat in ./.venv/lib/python3.11/site-packages (0.6.2)\r\n",
      "Requirement already satisfied: hmmlearn in ./.venv/lib/python3.11/site-packages (0.3.3)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: graphviz in ./.venv/lib/python3.11/site-packages (0.21)\r\n",
      "Requirement already satisfied: celluloid in ./.venv/lib/python3.11/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: pandas>=1.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (2.3.3)\r\n",
      "Requirement already satisfied: numpy>=1.9 in ./.venv/lib/python3.11/site-packages (from py-feat) (1.23.5)\r\n",
      "Requirement already satisfied: seaborn>=0.7.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (0.13.2)\r\n",
      "Requirement already satisfied: matplotlib>=2.1 in ./.venv/lib/python3.11/site-packages (from py-feat) (3.10.6)\r\n",
      "Requirement already satisfied: nltools>=0.5.1 in ./.venv/lib/python3.11/site-packages (from py-feat) (0.5.1)\r\n",
      "Requirement already satisfied: numexpr<2.8.5 in ./.venv/lib/python3.11/site-packages (from py-feat) (2.8.4)\r\n",
      "Requirement already satisfied: scikit-learn>=1.2 in ./.venv/lib/python3.11/site-packages (from py-feat) (1.7.2)\r\n",
      "Requirement already satisfied: pywavelets>=0.3.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (1.8.0)\r\n",
      "Requirement already satisfied: h5py>=2.7.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (3.14.0)\r\n",
      "Requirement already satisfied: Pillow>=6.0.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (11.3.0)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (from py-feat) (0.23.0)\r\n",
      "Requirement already satisfied: scikit-image>=0.19 in ./.venv/lib/python3.11/site-packages (from py-feat) (0.24.0)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from py-feat) (1.5.2)\r\n",
      "Requirement already satisfied: easing-functions in ./.venv/lib/python3.11/site-packages (from py-feat) (1.0.4)\r\n",
      "Requirement already satisfied: kornia in ./.venv/lib/python3.11/site-packages (from py-feat) (0.8.1)\r\n",
      "Requirement already satisfied: av>=9.2.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (15.1.0)\r\n",
      "Requirement already satisfied: xgboost>=1.6.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (3.0.5)\r\n",
      "Requirement already satisfied: scipy>=0.19 in ./.venv/lib/python3.11/site-packages (from hmmlearn) (1.15.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (25.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (2.9.0.post0)\r\n",
      "Requirement already satisfied: nibabel>=3.0.1 in ./.venv/lib/python3.11/site-packages (from nltools>=0.5.1->py-feat) (5.3.2)\r\n",
      "Requirement already satisfied: nilearn>=0.6.0 in ./.venv/lib/python3.11/site-packages (from nltools>=0.5.1->py-feat) (0.12.1)\r\n",
      "Requirement already satisfied: pynv in ./.venv/lib/python3.11/site-packages (from nltools>=0.5.1->py-feat) (0.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0->py-feat) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0->py-feat) (2025.2)\r\n",
      "Requirement already satisfied: networkx>=2.8 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (3.5)\r\n",
      "Requirement already satisfied: imageio>=2.33 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (2.37.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (2025.9.30)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (0.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.2->py-feat) (3.6.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.venv/lib/python3.11/site-packages (from xgboost>=1.6.0->py-feat) (2.27.3)\r\n",
      "Requirement already satisfied: kornia_rs>=0.1.9 in ./.venv/lib/python3.11/site-packages (from kornia->py-feat) (0.1.9)\r\n",
      "Requirement already satisfied: torch>=1.9.1 in ./.venv/lib/python3.11/site-packages (from kornia->py-feat) (2.8.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (1.14.0)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (2025.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (3.4.0)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.9.1->kornia->py-feat) (44.1.1)\r\n",
      "Requirement already satisfied: importlib-resources>=5.12 in ./.venv/lib/python3.11/site-packages (from nibabel>=3.0.1->nltools>=0.5.1->py-feat) (6.5.2)\r\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.11/site-packages (from nilearn>=0.6.0->nltools>=0.5.1->py-feat) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.25.0 in ./.venv/lib/python3.11/site-packages (from nilearn>=0.6.0->nltools>=0.5.1->py-feat) (2.32.5)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=2.1->py-feat) (1.17.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (2025.8.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.9.1->kornia->py-feat) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.9.1->kornia->py-feat) (3.0.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "feat: 0.6.2\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# System Graphviz requirement\n",
    "# If the DOT rendering below fails, install Graphviz:\n",
    "# - Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\n",
    "# - macOS: brew install graphviz\n",
    "# - Windows: Install Graphviz and add its bin directory to PATH"
   ],
   "id": "50ecbbd41b33d452"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T07:54:26.328707Z",
     "start_time": "2025-10-01T07:54:26.318763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import graphviz as _gvz_test\n",
    "from IPython.display import display\n",
    "import shutil as _shutil\n",
    "_dot = _gvz_test.Digraph()\n",
    "_dot.node('a'); _dot.node('b'); _dot.edge('a','b')\n",
    "if _shutil.which('dot'):\n",
    "    display(_dot)\n",
    "else:\n",
    "    print(\"Graphviz 'dot' executable not found on PATH. Install system Graphviz to render diagrams.\")\n",
    "    print(\"Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\")\n",
    "    print(\"macOS: brew install graphviz\")\n",
    "    print(\"Windows: Install Graphviz and add its bin directory to PATH\")\n",
    "    print(\"DOT source (you can paste this into an online Graphviz viewer):\")\n",
    "    print(_dot.source)\n"
   ],
   "id": "102fca6fc5ac127e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz 'dot' executable not found on PATH. Install system Graphviz to render diagrams.\n",
      "Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\n",
      "macOS: brew install graphviz\n",
      "Windows: Install Graphviz and add its bin directory to PATH\n",
      "DOT source (you can paste this into an online Graphviz viewer):\n",
      "digraph {\n",
      "\ta\n",
      "\tb\n",
      "\ta -> b\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Использование HMM на реальных данных\n",
    "\n",
    "Данный блокнот можно немного преувеличив минимальным прототипом аватара. Он демонстрирует выделение из видео признаков типа [F.A.C.S. Action Units]((https://en.wikipedia.org/wiki/Facial_Action_Coding_System#Method), обучение по ним скрытой марковской цепи (Hidden Markov Model, HMM) и анимацию предсказания.\n",
    "\n",
    "При анализе данных используется цепочка,\n",
    "```\n",
    "Image -> Land Marks -> FACS Action Units -> HMM\n",
    "```\n",
    "а в обратном ходе, при демонстрации аватаром лицевой анимации практически идентичная обратная\n",
    "```\n",
    "HMM -> FACS Action Units -> Animator -> Image\n",
    "```"
   ],
   "id": "d41fea14332d9e1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import scipy\n",
    "import itertools\n",
    "from feat.data import Fex\n",
    "from feat.utils.io import read_feat\n",
    "\n",
    "from hmmlearn import hmm\n",
    "import tqdm\n",
    "import tqdm.contrib.itertools\n",
    "\n",
    "import graphviz"
   ],
   "id": "22e055d887fa4b6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Выделение признаков\n",
    "\n",
    "Данные были получены при помощи библиотеки `py-feat`, которая умеет выделять из видео не только эмоции и опорные точки лица, но и Action Units системы FACS, что позволяет достаточно просто обратить преобразование и анимировать изображение."
   ],
   "id": "b3637cbe34b04d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from feat.detector import Detector\n",
    "from feat.utils.io import get_test_data_path\n",
    "import os\n",
    "\n",
    "enabled     = False         # False means do nothing\n",
    "video_path  = '/fix/me'\n",
    "csv_path    = '/fix/me/too'\n",
    "fps         = 25\n",
    "skip_frames = fps           # start detection every second\n",
    "face_detection_threshold = 0.95\n",
    "\n",
    "if enabled:\n",
    "    detector = Detector()\n",
    "    \n",
    "    video_prediction = detector.detect_video(\n",
    "        video_path,\n",
    "        skip_frames=skip_frames,\n",
    "        face_detection_threshold=face_detection_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving to csv {csv_path}\")\n",
    "    video_prediction.to_csv(csv_path)\n"
   ],
   "id": "1d2ce34264e00bfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Класс `feat.data.Fex`, экземпляром которого является video_prediction является оберткой над pandas DataFrame. Данные из него можно сохранять стандартными методами `pandas`, прочитать данные из csv обратно в `feat.data.Fex` можно при помощи `feat.utils.io.read_feat`.",
   "id": "1e14a85da91340c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = '../data/2019-11-05_17-34-46-Cam0.csv'\n",
    "\n",
    "data = read_feat(data_path)\n",
    "display(data)"
   ],
   "id": "c07d1bb8a8602f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "axes = data.emotions.plot()\n",
    "observation = data.aus\n",
    "labels = list(data.aus.columns)"
   ],
   "id": "10d1da038d2f101b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d87713b23d5684ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обучение HMM на FACS Action Units",
   "id": "b875047be8325425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_validate_fraction = .75   # train/validate\n",
    "min_state_n             = 2     # must be positive\n",
    "max_state_n             = 20    # must be grater than min_state_n\n",
    "attempts_to_fit_n      = 20    # number of attempts to fit a model\n",
    "                                # initial random state will different for each attempt\n",
    "show_progress_bar       = True  # show progress bar of overall training process\n",
    "print_log               = False # print a message per a trained model\n",
    "verbose                 = False # print verbose log of training for every model\n",
    "random_state            = 0     # random state seed, must be unsigned 32 bit integer\n",
    "n_samples_after         = 200   # number of states to generate after validating sequence\n",
    "raw_data_multiplier     = 100   # number to multiply data before conservation to integer \n",
    "\n",
    "X = np.array((raw_data_multiplier * (observation - np.min(observation))).astype(np.int_)) # for Poisson HMM all values must be positive\n",
    "# Sanity checks for PoissonHMM\n",
    "assert np.issubdtype(X.dtype, np.integer), \"X must be integer dtype\"\n",
    "assert X.min() >= 0, \"X must be non-negative\"\n",
    "\n",
    "train_validate_border = int(X.shape[0] * train_validate_fraction)\n",
    "\n",
    "X_train    = X[:train_validate_border]\n",
    "X_validate = X[train_validate_border:]\n",
    "\n",
    "scores = list()\n",
    "models = list()\n",
    "\n",
    "product = tqdm.contrib.itertools.product if show_progress_bar else itertools.product\n",
    "\n",
    "for idx, n_components in product(range(attempts_to_fit_n),\n",
    "                                     range(min_state_n, max_state_n)):\n",
    "        model = hmm.PoissonHMM(n_components = n_components,\n",
    "                               random_state=idx,\n",
    "                               verbose=verbose)\n",
    "        try:\n",
    "            model.fit(X_train)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(\"PoissonHMM fitting failed. Ensure observations are non-negative integers and sufficient in length.\") from e\n",
    "        models.append(model)\n",
    "        scores.append(model.score(X_validate))\n",
    "        if print_log:\n",
    "            print(f'Converged: {model.monitor_.converged}'\n",
    "                  f'\\tScore: {scores[-1]}')\n",
    "\n",
    "# get the best model\n",
    "model = models[np.argmax(scores)]\n",
    "print(f'The best model had a score of {max(scores)} and '\n",
    "      f'{model.n_components} components')\n",
    "\n",
    "# use the Viterbi algorithm to predict the most likely sequence of states\n",
    "# of the given the model for the train sequence\n",
    "states_train       = model.predict(X_train)\n",
    "# generate states with the model without any information from real data\n",
    "_, states_validate = model.sample(n_samples = len(X_validate) + n_samples_after,\n",
    "                              random_state = random_state)\n",
    "states = np.concatenate([states_train, states_validate])"
   ],
   "id": "766db001dc7175cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Демонстрации реальных и сгенерированных данных на одном графике",
   "id": "e8d09362f1ca575e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot model states over time\n",
    "X_label         = list(map(lambda label: f\"real {label}\", labels))\n",
    "fitted_label     = list(map(lambda label: f\"predicted {label}\", labels))\n",
    "first_au_num    = 16\n",
    "last_au_num     = 100 # feel free to set value greater then the maximum AU number\n",
    "aus_range       = range(first_au_num, min(X.shape[1], model.lambdas_[states].shape[1], last_au_num))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in aus_range:\n",
    "    line, = ax.plot(X[:,i],\n",
    "            linestyle=\"--\",\n",
    "            label = X_label[i])\n",
    "    ax.plot(model.lambdas_[states][:,i],\n",
    "            color = line.get_color(),\n",
    "            label = fitted_label[i])\n",
    "ax.axvline(x = train_validate_border,\n",
    "           linestyle = 'dotted',\n",
    "           color = 'gray',\n",
    "           label = 'train/validate border')\n",
    "ax.axvline(x = X.shape[0],\n",
    "           linestyle = 'dashed',\n",
    "           color = 'gray',\n",
    "           label = 'end of real data')\n",
    "ax.legend(loc = (1.05,0))\n",
    "ax.set_title('Predicted data and real data comparation')\n",
    "ax.set_xlabel('Frame number')\n",
    "plt.show()"
   ],
   "id": "7d77b6efcab776a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Визуализация внутреннего устройства HMM\n",
    "Внутренние состояния HMM организованы в граф, на ребрах которого отмечены вероятности переходов. Изобразим его матрицу смежности и сам граф."
   ],
   "id": "7c36a1657b22ed2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ax.imshow(model.transmat_, aspect='auto', cmap='spring')\n",
    "plot = ax.pcolormesh(model.transmat_,\n",
    "               edgecolors='white',\n",
    "               linewidth=1)\n",
    "fig.colorbar(plot)\n",
    "ax.invert_yaxis()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Transition Matrix')\n",
    "ax.set_xlabel('State To')\n",
    "ax.set_ylabel('State From')\n",
    "plt.show()"
   ],
   "id": "f337be92754c4b96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "min_prob_to_plot_an_edge = 0.01\n",
    "float_format = \"{0:0.2f}\"\n",
    "\n",
    "matrix = np.array(model.transmat_)\n",
    "\n",
    "assert(len(matrix.shape) == 2)\n",
    "assert(matrix.shape[0] == matrix.shape[1])\n",
    "\n",
    "dot = graphviz.Digraph('HMM graph', comment='HMM graph')\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    dot.node(str(i), str(i))\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        prob = matrix[i, j]\n",
    "        if prob > min_prob_to_plot_an_edge:\n",
    "            formated_prob = float_format.format(prob)\n",
    "            dot.edge(str(i), str(j), label = float_format.format(prob))\n",
    "from IPython.display import display\n",
    "import shutil as _shutil\n",
    "if _shutil.which('dot'):\n",
    "    display(dot)\n",
    "else:\n",
    "    print(\"Graphviz 'dot' executable not found on PATH. Install system Graphviz to render diagrams.\")\n",
    "    print(\"Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\")\n",
    "    print(\"macOS: brew install graphviz\")\n",
    "    print(\"Windows: Install Graphviz and add its bin directory to PATH\")\n",
    "    print(\"DOT source (you can paste this into an online Graphviz viewer):\")\n",
    "    print(dot.source)"
   ],
   "id": "3030f75fbdae54f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Анимация аватара\n",
    "Ключевое в данном блокноте именно возможность обратного хода, то есть генерации поведения после обучения. HMM генерирует временной ряд FACS Action Units, по которым анимируется аватар. Для простоты интеграции с jupiter используется примитивная картинка из библиотеки `py-feat`, однако существуют готовые решения, которые позволяют по Action Units 3d модели, о них ниже.\n",
    "\n",
    "## Анимация по FACS Action Units\n",
    "### FACS Avatar\n",
    "Докер контейнер, позволяющий анимировать при помощи FACS action units модель в Blender, Unity или MakeHuman. [GitHub](https://github.com/NumesSanguis/FACSvatar), лицензия LGPL. Был собран на базе нескольких других готовых проектов одним человеком, сейчас не поддерживается."
   ],
   "id": "b428590bd8abcf64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../why_FACS_control_trans.png')"
   ],
   "id": "236fb7d2b9edde85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### OpenFace FACS Unity Facial Animator\n",
    "Докер контейнер, который позволяет при помощи Action Units анимировать аватар на Unity.\n",
    "[Github](https://github.com/alexismorin/OpenFace-FACS-Unity-Facial-Animator), лицензия LGPL. Входит в состав FACS Avatar.\n",
    "\n",
    "### OpenFacs\n",
    "Код на C++, которые локально рендерит аватар и позволят динамически менять его выражение лица посылая новые Action Units по UDP. Проект единожды написанный для статьи и с тех пор заброшен, но запускается и работает. [GitHub](https://github.com/phuselab/openFACS), лицензия MIT."
   ],
   "id": "a38abe25a0feb164"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Анимация в этом блокноте\n",
    "Признаки выделялись только в каждом 25-ом кадре, то есть каждую секунду. Из-за этого движение глаз и рта хаотичны как на модели \"оригинале\", так и на аватаре."
   ],
   "id": "c0bc3b5821de49a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "from feat.plotting import plot_face\n",
    "\n",
    "def animate_au(fig, ax, aus, *args, **kwargs,):\n",
    "    camera = Camera(fig)\n",
    "    \n",
    "    for au in tqdm.tqdm(aus):\n",
    "        # ax.clear()\n",
    "        ax = plot_face(\n",
    "            model=None,\n",
    "            ax=ax,\n",
    "            au=au,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        camera.snap()\n",
    "    ani = camera.animate()\n",
    "    return ani"
   ],
   "id": "38151bebe1cb12e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "animation_fps = 10\n",
    "animation_dpi = 300\n",
    "animation_filename = \"input.gif\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "ani = animate_au(fig, ax, X / float(raw_data_multiplier))\n",
    "print(\"Person's face\")\n",
    "HTML(ani.to_jshtml())"
   ],
   "id": "e18810681fe5adf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "ani = animate_au(fig, ax, model.lambdas_[states] / float(raw_data_multiplier))\n",
    "print(\"Avatar's face\")\n",
    "HTML(ani.to_jshtml())"
   ],
   "id": "a1c24f710f15ff6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Эмоции как исходные данные для HMM\n",
    "В примере выше эмоции вообще никак не учитываются при моделировании. HMM принимает на вход только наблюдаемые Action Units, эмоций для нее не существует.\n",
    "\n",
    "Может оказаться, что лучше все таки концепцию эмоций использовать. Тогда пути данных в прямом и обратном направлении будут такими:\n",
    "```\n",
    "Image -> Land Marks -> FACS Action Units -> Emotions -> HMM\n",
    "```\n",
    "```\n",
    "HMM -> Emotions -> FACS Action Units -> Animator -> Image\n",
    "```\n",
    "## Распознавание эмоций\n",
    "Эмоции по FACS Action Units можно распознавать при помощи ML моделей, так и делает библиотека `py-feat`. Проблема в том, такое преобразование получается необратимым. По Action Units эмоции восстановить можно, а как из модели достать обратное преобразование не ясно.\n",
    "\n",
    "Будем использовать другой, более простой и не связанный с ML\n",
    "подход. Назовем эмоцией подпространство в пространстве action\n",
    "units. Выражение лица описывается вектором в Action Units пространстве. Степень выражения соответствия выражения лица эмоции равна углу между подпространством эмоции и вектором выражения лица.\n",
    "\n",
    "Некоторые эмоции могут иметь несколько способов выражения. Так удивляясь человек может приоткрыть рот, а может и нет. Правильнее сказать, что что эмоция -- класс эквивалентности подпространств в пространстве Action Units. Но будем для простоты считать, удивление с открытым ртом и закрытым -- разные эмоции."
   ],
   "id": "100b5dd4c030a087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Refactoring, Выделить связанные с эмоциями функции и структуры данных в класс.",
   "id": "3b2114415f19cfa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def auName2auNum(name : str):\n",
    "    return int(name[-2:])\n",
    "def auNum2auName(number : int):\n",
    "    return f\"AU{int(number):02d}\"\n",
    "def test_auName2auNum_auNum2auName():\n",
    "    for name in list(data.aus.columns):\n",
    "        number = auName2auNum(name)\n",
    "        name_test = auNum2auName(number)\n",
    "        if (name_test != name):\n",
    "            print(f\"Failed {name} -> {number} -> {name_test}\")\n",
    "            assert(False)\n",
    "test_auName2auNum_auNum2auName()"
   ],
   "id": "84ac60975b1d3c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Данные об эмоциях-подпространствах возьмем из книги Paul Ekman, Wallace V. Friesen, Joseph C. Hager, \"Facial Action Coding System Investigator’s Guide\". В книге вводится интуитивно понятный DSL для их описания в виде формальных сумм. Звездочка (*) около номера Action Unit означает, что этот Action Unit может быть выражен в любой степени.",
   "id": "52417198d64e6154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# table from Facial Action Coding System Investigator’s Guide, by Paul Ekman, Wallace V. Friesen, Joseph C. Hager\n",
    "emotions = {\n",
    "    'anger' : {\n",
    "        '4+5*+7+10*+22+23+25',\n",
    "        '4+5*+7+23+25',\n",
    "        '4+5*+7+10*+22+23+26',\n",
    "        '4+5*+7+23+26',\n",
    "        '4+5*+7+17+23',\n",
    "        '4+5*+7+17+24',\n",
    "        '4+5*+7+23',\n",
    "        '4+5*+7+24',\n",
    "    },\n",
    "    'disgust' : {\n",
    "        '9',\n",
    "        '9+16+15',\n",
    "        '9+16+26',\n",
    "        '9+17',\n",
    "        '10*',\n",
    "        '10*+16+26',\n",
    "        '10*+16+25',\n",
    "        '10+17',\n",
    "    },\n",
    "    'fear' : {\n",
    "        '1+2+4+5*+20*+25',\n",
    "        '1+2+4+5*+25',\n",
    "        '1+2+4+5*+20*+26',\n",
    "        '1+2+4+5*+26',\n",
    "        '1+2+4+5*+20*+27',\n",
    "        '1+2+4+5*+27',\n",
    "    }, \n",
    "    'happiness' : {\n",
    "        '6+12*',\n",
    "        '12',\n",
    "    },\n",
    "    'sadness' : {\n",
    "        '1+4+11+15',\n",
    "        '1+4+15*',\n",
    "        '6+15*',\n",
    "    },\n",
    "    'surprise' : {\n",
    "        '1+2+5+26',\n",
    "        '1+2+5+27',\n",
    "    },\n",
    "    #'neutral' : {}\n",
    "}"
   ],
   "id": "8d0c0e93d578c91d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подпространство эмоции будем описывать при помощи ортогонально проецирующей матрицы. `parseAdditiveCombination` конвертируют строку формальной суммы в матрицу-проектор на подпространство.",
   "id": "3fd49c7cb94d8a2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parseAdditiveCombination(combination: str, aus_space: list):\n",
    "    au_strs = combination.split('+')\n",
    "    N = len(aus_space)\n",
    "    if N == 0:\n",
    "        return None\n",
    "    # Build index for O(1) lookup\n",
    "    idx_by_au = {name: i for i, name in enumerate(aus_space)}\n",
    "\n",
    "    vector_sum = np.zeros(N)\n",
    "    direct_sum = []  # rows = basis vectors\n",
    "\n",
    "    for au_str in au_strs:\n",
    "        try:\n",
    "            add_to_direct_sum = (au_str[-1] == '*')\n",
    "            if add_to_direct_sum:\n",
    "                au_str = au_str[:-1]  # remove asterisk\n",
    "            au_name = auNum2auName(int(au_str))\n",
    "            idx = idx_by_au.get(au_name)\n",
    "            if idx is None:\n",
    "                # AU is not present in the current space; skip it\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid term \\\"{au_str}\\\" in additive combination \\\"{combination}\\\"\") from e\n",
    "\n",
    "        vector = np.zeros(N)\n",
    "        vector[idx] = 1\n",
    "\n",
    "        if add_to_direct_sum:\n",
    "            direct_sum.append(vector)\n",
    "        else:\n",
    "            vector_sum += vector\n",
    "\n",
    "    norm = np.linalg.norm(vector_sum)\n",
    "    if norm > 0:\n",
    "        vector_sum = vector_sum / norm\n",
    "        direct_sum.append(vector_sum)\n",
    "\n",
    "    if len(direct_sum) == 0:\n",
    "        return None\n",
    "    return np.stack(direct_sum, axis=0)\n",
    "\n",
    "def test_parseAdditiveCombination():\n",
    "    N = 3\n",
    "    au_labels = list(map(auNum2auName, np.arange(N) + 1))\n",
    "    ones = np.ones(N)\n",
    "    assert np.allclose(parseAdditiveCombination('1*+2*+3*', au_labels), np.eye(N))\n",
    "    expected = (ones / np.linalg.norm(ones))[None, :]\n",
    "    assert np.allclose(parseAdditiveCombination('1+2+3', au_labels), expected)\n",
    "    assert parseAdditiveCombination('42', au_labels) is None\n",
    "\n",
    "test_parseAdditiveCombination()"
   ],
   "id": "2f990d2fcac3398b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Как уже было сказано выше, степень варажения соответствия выражения лица эмоции равнауглу между подпространством эмоции и ветором выражения лица.",
   "id": "e961f04cd9a13d18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def subspace_angles_rows(A_rows: np.ndarray, B_rows: np.ndarray):\n",
    "    \"\"\"Compute principal angles between two subspaces with bases given in rows.\"\"\"\n",
    "    A = np.atleast_2d(A_rows)\n",
    "    B = np.atleast_2d(B_rows)\n",
    "    return scipy.linalg.subspace_angles(A.T, B.T)\n",
    "\n",
    "\n",
    "def emotion_value(emotion_projector: np.ndarray, face_expression: np.ndarray):\n",
    "    \"\"\"Angles between an emotion subspace (rows as basis) and a face-expression vector.\"\"\"\n",
    "    # If a vector is provided, make it a single row; if already a matrix, use as-is\n",
    "    if isinstance(face_expression, np.ndarray) and face_expression.ndim == 1:\n",
    "        v_rows = face_expression.reshape(1, -1)\n",
    "    else:\n",
    "        v_rows = np.atleast_2d(face_expression)\n",
    "    return subspace_angles_rows(emotion_projector, v_rows)"
   ],
   "id": "2a74b99dcffca9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "emotion_spaces = {}\n",
    "for emotion_name, sum_strs in emotions.items():\n",
    "    subspaces = []\n",
    "    for sum_str in sum_strs:\n",
    "        subspace = parseAdditiveCombination(sum_str, list(data.aus.columns))\n",
    "        if subspace is None:\n",
    "            continue\n",
    "        if len(subspaces) == 0:\n",
    "            subspaces.append(subspace)\n",
    "            continue\n",
    "        # check if subspace has already been added (near-zero principal angle)\n",
    "        angles_list = [subspace_angles_rows(subspace, existing) for existing in subspaces]\n",
    "        near_zero = any((angles_arr < 1e-8).any() for angles_arr in angles_list)\n",
    "        if not near_zero:\n",
    "            subspaces.append(subspace)\n",
    "    emotion_spaces[emotion_name] = subspaces\n",
    "\n",
    "# display(emotion_spaces)"
   ],
   "id": "a9ac4eed3423b396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys, numpy, pandas, scipy, matplotlib, sklearn, hmmlearn\n",
    "import feat\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pandas:\", pandas.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"py-feat:\", getattr(feat, \"__version__\", \"n/a\"))\n",
    "print(\"hmmlearn:\", hmmlearn.__version__)"
   ],
   "id": "1805037f25d44147"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
