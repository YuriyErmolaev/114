{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:12:54.893258Z",
     "start_time": "2025-10-01T09:12:54.227337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, site, platform\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Python ver:\", platform.python_version())\n",
    "!{sys.executable} -m pip --version\n",
    "!{sys.executable} -m pip show py-feat hmmlearn tqdm graphviz celluloid\n"
   ],
   "id": "151a0c79975f5e38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/bin/python\n",
      "Python ver: 3.11.13\n",
      "pip 24.3.1 from /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages/pip (python 3.11)\r\n",
      "Name: py-feat\r\n",
      "Version: 0.6.2\r\n",
      "Summary: Facial Expression Analysis Toolbox\r\n",
      "Home-page: https://github.com/cosanlab/py-feat\r\n",
      "Author: Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Eshin Jolly, Luke Chang\r\n",
      "Author-email: jcheong0428@gmail.com, eshin.jolly@gmail.com, luke.j.chang@dartmouth.edu\r\n",
      "License: MIT license\r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: av, celluloid, easing-functions, h5py, joblib, kornia, matplotlib, nltools, numexpr, numpy, pandas, Pillow, pywavelets, scikit-image, scikit-learn, seaborn, torchvision, tqdm, xgboost\r\n",
      "Required-by: \r\n",
      "---\r\n",
      "Name: hmmlearn\r\n",
      "Version: 0.3.3\r\n",
      "Summary: Hidden Markov Models in Python with scikit-learn like API\r\n",
      "Home-page: \r\n",
      "Author: David Cournapeau, Fabian Pedregosa, Gael Varoquaux, Sergei Lebedev, Antony Lee, Matthew Danielson\r\n",
      "Author-email: \r\n",
      "License: \r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: numpy, scikit-learn, scipy\r\n",
      "Required-by: \r\n",
      "---\r\n",
      "Name: tqdm\r\n",
      "Version: 4.67.1\r\n",
      "Summary: Fast, Extensible Progress Meter\r\n",
      "Home-page: https://tqdm.github.io\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MPL-2.0 AND MIT\r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: py-feat\r\n",
      "---\r\n",
      "Name: graphviz\r\n",
      "Version: 0.21\r\n",
      "Summary: Simple Python interface for Graphviz\r\n",
      "Home-page: https://github.com/xflr6/graphviz\r\n",
      "Author: \r\n",
      "Author-email: Sebastian Bank <sebastian.bank@uni-leipzig.de>\r\n",
      "License: \r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: \r\n",
      "---\r\n",
      "Name: celluloid\r\n",
      "Version: 0.2.0\r\n",
      "Summary: Easy matplotlib animation.\r\n",
      "Home-page: https://github.com/jwkvam/celluloid\r\n",
      "Author: Jacques Kvam\r\n",
      "Author-email: jwkvam+pypi@gmail.com\r\n",
      "License: UNKNOWN\r\n",
      "Location: /home/yuriy/Projects/avatar_detectionsystem-2_common/114/apiapp/server/.venv/lib/python3.11/site-packages\r\n",
      "Requires: matplotlib\r\n",
      "Required-by: py-feat\r\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:12:58.225617Z",
     "start_time": "2025-10-01T09:12:56.490814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install -U py-feat hmmlearn tqdm graphviz celluloid\n",
    "# quick check and SciPy compatibility shim for py-feat expecting scipy.integrate.simps\n",
    "import scipy\n",
    "try:\n",
    "    from scipy import integrate as _integrate\n",
    "    if not hasattr(_integrate, \"simps\"):\n",
    "        from scipy.integrate import simpson as _simpson\n",
    "        setattr(_integrate, \"simps\", _simpson)\n",
    "except Exception as _e:\n",
    "    print(\"Warning: could not alias scipy.integrate.simpson to simps:\", _e)\n",
    "# Compatibility shim for environments where lib2to3 is unavailable (e.g., Py3.12+)\n",
    "try:\n",
    "    import sys as _sys, types as _types\n",
    "    if 'lib2to3.pytree' not in _sys.modules:\n",
    "        _lib2to3 = _types.ModuleType('lib2to3')\n",
    "        _pytree = _types.ModuleType('lib2to3.pytree')\n",
    "        def convert(node):  # minimal stub used by py-feat's ResMaskNet wrapper\n",
    "            return node\n",
    "        _pytree.convert = convert\n",
    "        _lib2to3.pytree = _pytree\n",
    "        _sys.modules['lib2to3'] = _lib2to3\n",
    "        _sys.modules['lib2to3.pytree'] = _pytree\n",
    "except Exception as _e:\n",
    "    print(\"Warning: could not create lib2to3 compatibility shim:\", _e)\n",
    "import feat, hmmlearn, graphviz, celluloid\n",
    "# Compatibility: ensure feat.Fex exists for py-feat versions that don't export it at top-level\n",
    "try:\n",
    "    if not hasattr(feat, \"Fex\"):\n",
    "        from feat.data import Fex as _Fex\n",
    "        setattr(feat, \"Fex\", _Fex)\n",
    "except Exception as _e:\n",
    "    print(\"Warning: could not alias feat.data.Fex to feat.Fex:\", _e)\n",
    "print(\"feat:\", getattr(feat, \"__version__\", \"n/a\"))\n"
   ],
   "id": "1611c44a76c5f0db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-feat in ./.venv/lib/python3.11/site-packages (0.6.2)\r\n",
      "Requirement already satisfied: hmmlearn in ./.venv/lib/python3.11/site-packages (0.3.3)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: graphviz in ./.venv/lib/python3.11/site-packages (0.21)\r\n",
      "Requirement already satisfied: celluloid in ./.venv/lib/python3.11/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: pandas>=1.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (2.3.3)\r\n",
      "Requirement already satisfied: numpy>=1.9 in ./.venv/lib/python3.11/site-packages (from py-feat) (1.23.5)\r\n",
      "Requirement already satisfied: seaborn>=0.7.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (0.13.2)\r\n",
      "Requirement already satisfied: matplotlib>=2.1 in ./.venv/lib/python3.11/site-packages (from py-feat) (3.10.6)\r\n",
      "Requirement already satisfied: nltools>=0.5.1 in ./.venv/lib/python3.11/site-packages (from py-feat) (0.5.1)\r\n",
      "Requirement already satisfied: numexpr<2.8.5 in ./.venv/lib/python3.11/site-packages (from py-feat) (2.8.4)\r\n",
      "Requirement already satisfied: scikit-learn>=1.2 in ./.venv/lib/python3.11/site-packages (from py-feat) (1.7.2)\r\n",
      "Requirement already satisfied: pywavelets>=0.3.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (1.8.0)\r\n",
      "Requirement already satisfied: h5py>=2.7.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (3.14.0)\r\n",
      "Requirement already satisfied: Pillow>=6.0.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (11.3.0)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (from py-feat) (0.23.0)\r\n",
      "Requirement already satisfied: scikit-image>=0.19 in ./.venv/lib/python3.11/site-packages (from py-feat) (0.24.0)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from py-feat) (1.5.2)\r\n",
      "Requirement already satisfied: easing-functions in ./.venv/lib/python3.11/site-packages (from py-feat) (1.0.4)\r\n",
      "Requirement already satisfied: kornia in ./.venv/lib/python3.11/site-packages (from py-feat) (0.8.1)\r\n",
      "Requirement already satisfied: av>=9.2.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (15.1.0)\r\n",
      "Requirement already satisfied: xgboost>=1.6.0 in ./.venv/lib/python3.11/site-packages (from py-feat) (3.0.5)\r\n",
      "Requirement already satisfied: scipy>=0.19 in ./.venv/lib/python3.11/site-packages (from hmmlearn) (1.15.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (25.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib>=2.1->py-feat) (2.9.0.post0)\r\n",
      "Requirement already satisfied: nibabel>=3.0.1 in ./.venv/lib/python3.11/site-packages (from nltools>=0.5.1->py-feat) (5.3.2)\r\n",
      "Requirement already satisfied: nilearn>=0.6.0 in ./.venv/lib/python3.11/site-packages (from nltools>=0.5.1->py-feat) (0.12.1)\r\n",
      "Requirement already satisfied: pynv in ./.venv/lib/python3.11/site-packages (from nltools>=0.5.1->py-feat) (0.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0->py-feat) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0->py-feat) (2025.2)\r\n",
      "Requirement already satisfied: networkx>=2.8 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (3.5)\r\n",
      "Requirement already satisfied: imageio>=2.33 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (2.37.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (2025.9.30)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.19->py-feat) (0.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.2->py-feat) (3.6.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.venv/lib/python3.11/site-packages (from xgboost>=1.6.0->py-feat) (2.27.3)\r\n",
      "Requirement already satisfied: kornia_rs>=0.1.9 in ./.venv/lib/python3.11/site-packages (from kornia->py-feat) (0.1.9)\r\n",
      "Requirement already satisfied: torch>=1.9.1 in ./.venv/lib/python3.11/site-packages (from kornia->py-feat) (2.8.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (1.14.0)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (2025.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.9.1->kornia->py-feat) (3.4.0)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.9.1->kornia->py-feat) (44.1.1)\r\n",
      "Requirement already satisfied: importlib-resources>=5.12 in ./.venv/lib/python3.11/site-packages (from nibabel>=3.0.1->nltools>=0.5.1->py-feat) (6.5.2)\r\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.11/site-packages (from nilearn>=0.6.0->nltools>=0.5.1->py-feat) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.25.0 in ./.venv/lib/python3.11/site-packages (from nilearn>=0.6.0->nltools>=0.5.1->py-feat) (2.32.5)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=2.1->py-feat) (1.17.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools>=0.5.1->py-feat) (2025.8.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.9.1->kornia->py-feat) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.9.1->kornia->py-feat) (3.0.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "feat: 0.6.2\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# System Graphviz requirement\n",
    "# If the DOT rendering below fails, install Graphviz:\n",
    "# - Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\n",
    "# - macOS: brew install graphviz\n",
    "# - Windows: Install Graphviz and add its bin directory to PATH"
   ],
   "id": "f0e2f53f997414c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:13:03.205348Z",
     "start_time": "2025-10-01T09:13:03.149047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import graphviz as _gvz_test\n",
    "from IPython.display import display\n",
    "import shutil as _shutil\n",
    "_dot = _gvz_test.Digraph()\n",
    "_dot.node('a'); _dot.node('b'); _dot.edge('a','b')\n",
    "if _shutil.which('dot'):\n",
    "    display(_dot)\n",
    "else:\n",
    "    print(\"Graphviz 'dot' executable not found on PATH. Install system Graphviz to render diagrams.\")\n",
    "    print(\"Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\")\n",
    "    print(\"macOS: brew install graphviz\")\n",
    "    print(\"Windows: Install Graphviz and add its bin directory to PATH\")\n",
    "    print(\"DOT source (you can paste this into an online Graphviz viewer):\")\n",
    "    print(_dot.source)\n"
   ],
   "id": "e64a42484a4ef42",
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"62pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 62.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 58,-112 58,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n</g>\n<!-- b -->\n<g id=\"node2\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n</g>\n<!-- a&#45;&gt;b -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;b</title>\n<path fill=\"none\" stroke=\"black\" d=\"M27,-71.7C27,-63.98 27,-54.71 27,-46.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"30.5,-46.1 27,-36.1 23.5,-46.1 30.5,-46.1\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x78b0a0c42750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Использование HMM на реальных данных\n",
    "\n",
    "Данный блокнот можно немного преувеличив минимальным прототипом аватара. Он демонстрирует выделение из видео признаков типа [F.A.C.S. Action Units]((https://en.wikipedia.org/wiki/Facial_Action_Coding_System#Method), обучение по ним скрытой марковской цепи (Hidden Markov Model, HMM) и анимацию предсказания.\n",
    "\n",
    "При анализе данных используется цепочка,\n",
    "```\n",
    "Image -> Land Marks -> FACS Action Units -> HMM\n",
    "```\n",
    "а в обратном ходе, при демонстрации аватаром лицевой анимации практически идентичная обратная\n",
    "```\n",
    "HMM -> FACS Action Units -> Animator -> Image\n",
    "```"
   ],
   "id": "2eafdd41a5e79da1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:13:08.761310Z",
     "start_time": "2025-10-01T09:13:08.759117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import scipy\n",
    "import itertools\n",
    "from feat.data import Fex\n",
    "from feat.utils.io import read_feat\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from tqdm.std import tqdm\n",
    "import tqdm.contrib.itertools as tqdm_itertools\n",
    "\n",
    "import graphviz"
   ],
   "id": "7c9e65281a90b810",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Выделение признаков\n",
    "\n",
    "Данные были получены при помощи библиотеки `py-feat`, которая умеет выделять из видео не только эмоции и опорные точки лица, но и Action Units системы FACS, что позволяет достаточно просто обратить преобразование и анимировать изображение."
   ],
   "id": "7471ce9bc8780147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:13:13.801097Z",
     "start_time": "2025-10-01T09:13:13.797618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feat.detector import Detector\n",
    "from feat.utils.io import get_test_data_path\n",
    "import os\n",
    "\n",
    "enabled     = False         # False means do nothing\n",
    "video_path  = '/fix/me'\n",
    "csv_path    = '/fix/me/too'\n",
    "fps         = 25\n",
    "skip_frames = fps           # start detection every second\n",
    "face_detection_threshold = 0.95\n",
    "\n",
    "if enabled:\n",
    "    detector = Detector()\n",
    "    \n",
    "    video_prediction = detector.detect_video(\n",
    "        video_path,\n",
    "        skip_frames=skip_frames,\n",
    "        face_detection_threshold=face_detection_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving to csv {csv_path}\")\n",
    "    video_prediction.to_csv(csv_path)\n"
   ],
   "id": "bf42da1c06375b50",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Класс `feat.data.Fex`, экземпляром которого является video_prediction является оберткой над pandas DataFrame. Данные из него можно сохранять стандартными методами `pandas`, прочитать данные из csv обратно в `feat.data.Fex` можно при помощи `feat.utils.io.read_feat`.",
   "id": "5a7f1b4d50ab501b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T09:13:17.903854Z",
     "start_time": "2025-10-01T09:13:17.766438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = 'data/2019-11-05_17-34-46-Cam0.csv'\n",
    "\n",
    "# Robustly read FEX CSV: try py-feat's read_feat; if it fails due to missing feat.Fex top-level,\n",
    "# fall back to constructing Fex directly from feat.data using constants from feat.utils.io.\n",
    "try:\n",
    "    data = read_feat(data_path)\n",
    "except AttributeError as e:\n",
    "    import pandas as pd\n",
    "    import feat as _feat\n",
    "    # Ensure feat.Fex exists for compatibility with py-feat 0.6.x\n",
    "    try:\n",
    "        Fex = getattr(_feat, \"Fex\")\n",
    "    except AttributeError:\n",
    "        from feat.data import Fex as _Fex\n",
    "        setattr(_feat, \"Fex\", _Fex)\n",
    "        Fex = _Fex\n",
    "    from feat.utils.io import (\n",
    "        FEAT_EMOTION_COLUMNS,\n",
    "        FEAT_FACEBOX_COLUMNS,\n",
    "        FEAT_TIME_COLUMNS,\n",
    "        openface_2d_landmark_columns,\n",
    "    )\n",
    "    d = pd.read_csv(data_path)\n",
    "    au_columns = [col for col in d.columns if \"AU\" in col]\n",
    "    data = Fex(\n",
    "        d,\n",
    "        filename=data_path,\n",
    "        au_columns=au_columns,\n",
    "        emotion_columns=FEAT_EMOTION_COLUMNS,\n",
    "        landmark_columns=openface_2d_landmark_columns,\n",
    "        facebox_columns=FEAT_FACEBOX_COLUMNS,\n",
    "        time_columns=FEAT_TIME_COLUMNS,\n",
    "        facepose_columns=[\"Pitch\", \"Roll\", \"Yaw\"],\n",
    "        detector=\"Feat\",\n",
    "    )\n",
    "\n",
    "display(data)"
   ],
   "id": "38e60e4f1ff80a81",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     frame   FaceRectX   FaceRectY  FaceRectWidth  FaceRectHeight  FaceScore  \\\n",
       "0        0  552.228447  234.907543     294.547830      430.743182   0.998109   \n",
       "1       25  547.547666  232.236796     297.105293      436.905043   0.998266   \n",
       "2       50  547.085259  230.146992     298.485543      439.968903   0.998377   \n",
       "3       75  543.871207  231.905862     297.703602      436.730056   0.998465   \n",
       "4      100  545.973171  233.737574     295.826781      435.552277   0.998256   \n",
       "..     ...         ...         ...            ...             ...        ...   \n",
       "418  10450  606.454607  250.198161     304.745026      414.537910   0.997687   \n",
       "419  10475  609.587960  248.944963     291.213764      417.413147   0.997802   \n",
       "420  10500  608.597204  248.781753     292.189447      417.364380   0.997881   \n",
       "421  10525  604.751495  245.466940     304.926145      422.061784   0.997693   \n",
       "422  10550  596.025610  242.016908     284.675379      409.019197   0.997574   \n",
       "\n",
       "            x_0         x_1         x_2         x_3  ...  Identity_506  \\\n",
       "0    533.474727  536.409884  543.464834  554.705615  ...      0.084257   \n",
       "1    532.898309  535.535572  542.204305  552.920773  ...      0.087330   \n",
       "2    532.875874  535.769899  542.634632  553.310688  ...      0.091887   \n",
       "3    530.851597  533.680875  540.380884  551.095091  ...      0.089667   \n",
       "4    529.293922  532.328330  539.165094  549.865282  ...      0.089284   \n",
       "..          ...         ...         ...         ...  ...           ...   \n",
       "418  596.028282  591.680086  592.152849  598.197547  ...      0.090237   \n",
       "419  595.238209  591.018746  591.762303  598.121249  ...      0.096973   \n",
       "420  595.934896  591.574591  592.130424  598.350501  ...      0.095804   \n",
       "421  594.397516  590.673944  591.774885  597.977747  ...      0.093611   \n",
       "422  574.961607  573.508056  576.305429  583.331624  ...      0.086807   \n",
       "\n",
       "     Identity_507  Identity_508  Identity_509  Identity_510  Identity_511  \\\n",
       "0       -0.036940      0.042500     -0.016232      0.038723      0.074294   \n",
       "1       -0.027688      0.037983     -0.018535      0.045254      0.078598   \n",
       "2       -0.026540      0.039663     -0.020008      0.047929      0.074559   \n",
       "3       -0.027624      0.039284     -0.017401      0.050178      0.076037   \n",
       "4       -0.026742      0.037668     -0.018124      0.045886      0.075792   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "418     -0.032405      0.044863     -0.023863      0.047819      0.082969   \n",
       "419     -0.031550      0.042423     -0.028677      0.039181      0.075109   \n",
       "420     -0.030461      0.037923     -0.019580      0.038524      0.081636   \n",
       "421     -0.040855      0.052874     -0.019804      0.035093      0.067575   \n",
       "422     -0.034171      0.026519     -0.042282      0.061826      0.079765   \n",
       "\n",
       "     Identity_512                                          input  frame.1  \\\n",
       "0       -0.045343  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4        0   \n",
       "1       -0.049255  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4       25   \n",
       "2       -0.049308  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4       50   \n",
       "3       -0.052258  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4       75   \n",
       "4       -0.044758  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4      100   \n",
       "..            ...                                            ...      ...   \n",
       "418     -0.034510  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4    10450   \n",
       "419     -0.040604  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4    10475   \n",
       "420     -0.029180  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4    10500   \n",
       "421     -0.028447  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4    10525   \n",
       "422     -0.035243  /storage/avatar2/2019-11-05_17-34-46/Cam0.mp4    10550   \n",
       "\n",
       "     approx_time  \n",
       "0          00:00  \n",
       "1          00:01  \n",
       "2          00:02  \n",
       "3          00:03  \n",
       "4          00:04  \n",
       "..           ...  \n",
       "418        06:58  \n",
       "419        06:59  \n",
       "420        07:00  \n",
       "421        07:01  \n",
       "422        07:02  \n",
       "\n",
       "[423 rows x 688 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>FaceRectX</th>\n",
       "      <th>FaceRectY</th>\n",
       "      <th>FaceRectWidth</th>\n",
       "      <th>FaceRectHeight</th>\n",
       "      <th>FaceScore</th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Identity_506</th>\n",
       "      <th>Identity_507</th>\n",
       "      <th>Identity_508</th>\n",
       "      <th>Identity_509</th>\n",
       "      <th>Identity_510</th>\n",
       "      <th>Identity_511</th>\n",
       "      <th>Identity_512</th>\n",
       "      <th>input</th>\n",
       "      <th>frame.1</th>\n",
       "      <th>approx_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>552.228447</td>\n",
       "      <td>234.907543</td>\n",
       "      <td>294.547830</td>\n",
       "      <td>430.743182</td>\n",
       "      <td>0.998109</td>\n",
       "      <td>533.474727</td>\n",
       "      <td>536.409884</td>\n",
       "      <td>543.464834</td>\n",
       "      <td>554.705615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>-0.036940</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>-0.016232</td>\n",
       "      <td>0.038723</td>\n",
       "      <td>0.074294</td>\n",
       "      <td>-0.045343</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>547.547666</td>\n",
       "      <td>232.236796</td>\n",
       "      <td>297.105293</td>\n",
       "      <td>436.905043</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>532.898309</td>\n",
       "      <td>535.535572</td>\n",
       "      <td>542.204305</td>\n",
       "      <td>552.920773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087330</td>\n",
       "      <td>-0.027688</td>\n",
       "      <td>0.037983</td>\n",
       "      <td>-0.018535</td>\n",
       "      <td>0.045254</td>\n",
       "      <td>0.078598</td>\n",
       "      <td>-0.049255</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>25</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>547.085259</td>\n",
       "      <td>230.146992</td>\n",
       "      <td>298.485543</td>\n",
       "      <td>439.968903</td>\n",
       "      <td>0.998377</td>\n",
       "      <td>532.875874</td>\n",
       "      <td>535.769899</td>\n",
       "      <td>542.634632</td>\n",
       "      <td>553.310688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091887</td>\n",
       "      <td>-0.026540</td>\n",
       "      <td>0.039663</td>\n",
       "      <td>-0.020008</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>0.074559</td>\n",
       "      <td>-0.049308</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>50</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>543.871207</td>\n",
       "      <td>231.905862</td>\n",
       "      <td>297.703602</td>\n",
       "      <td>436.730056</td>\n",
       "      <td>0.998465</td>\n",
       "      <td>530.851597</td>\n",
       "      <td>533.680875</td>\n",
       "      <td>540.380884</td>\n",
       "      <td>551.095091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089667</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.039284</td>\n",
       "      <td>-0.017401</td>\n",
       "      <td>0.050178</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>-0.052258</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>75</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>545.973171</td>\n",
       "      <td>233.737574</td>\n",
       "      <td>295.826781</td>\n",
       "      <td>435.552277</td>\n",
       "      <td>0.998256</td>\n",
       "      <td>529.293922</td>\n",
       "      <td>532.328330</td>\n",
       "      <td>539.165094</td>\n",
       "      <td>549.865282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089284</td>\n",
       "      <td>-0.026742</td>\n",
       "      <td>0.037668</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>0.045886</td>\n",
       "      <td>0.075792</td>\n",
       "      <td>-0.044758</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>100</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>10450</td>\n",
       "      <td>606.454607</td>\n",
       "      <td>250.198161</td>\n",
       "      <td>304.745026</td>\n",
       "      <td>414.537910</td>\n",
       "      <td>0.997687</td>\n",
       "      <td>596.028282</td>\n",
       "      <td>591.680086</td>\n",
       "      <td>592.152849</td>\n",
       "      <td>598.197547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090237</td>\n",
       "      <td>-0.032405</td>\n",
       "      <td>0.044863</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.082969</td>\n",
       "      <td>-0.034510</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>10450</td>\n",
       "      <td>06:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>10475</td>\n",
       "      <td>609.587960</td>\n",
       "      <td>248.944963</td>\n",
       "      <td>291.213764</td>\n",
       "      <td>417.413147</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>595.238209</td>\n",
       "      <td>591.018746</td>\n",
       "      <td>591.762303</td>\n",
       "      <td>598.121249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096973</td>\n",
       "      <td>-0.031550</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>-0.028677</td>\n",
       "      <td>0.039181</td>\n",
       "      <td>0.075109</td>\n",
       "      <td>-0.040604</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>10475</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>10500</td>\n",
       "      <td>608.597204</td>\n",
       "      <td>248.781753</td>\n",
       "      <td>292.189447</td>\n",
       "      <td>417.364380</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>595.934896</td>\n",
       "      <td>591.574591</td>\n",
       "      <td>592.130424</td>\n",
       "      <td>598.350501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095804</td>\n",
       "      <td>-0.030461</td>\n",
       "      <td>0.037923</td>\n",
       "      <td>-0.019580</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>0.081636</td>\n",
       "      <td>-0.029180</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>10500</td>\n",
       "      <td>07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>10525</td>\n",
       "      <td>604.751495</td>\n",
       "      <td>245.466940</td>\n",
       "      <td>304.926145</td>\n",
       "      <td>422.061784</td>\n",
       "      <td>0.997693</td>\n",
       "      <td>594.397516</td>\n",
       "      <td>590.673944</td>\n",
       "      <td>591.774885</td>\n",
       "      <td>597.977747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093611</td>\n",
       "      <td>-0.040855</td>\n",
       "      <td>0.052874</td>\n",
       "      <td>-0.019804</td>\n",
       "      <td>0.035093</td>\n",
       "      <td>0.067575</td>\n",
       "      <td>-0.028447</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>10525</td>\n",
       "      <td>07:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>10550</td>\n",
       "      <td>596.025610</td>\n",
       "      <td>242.016908</td>\n",
       "      <td>284.675379</td>\n",
       "      <td>409.019197</td>\n",
       "      <td>0.997574</td>\n",
       "      <td>574.961607</td>\n",
       "      <td>573.508056</td>\n",
       "      <td>576.305429</td>\n",
       "      <td>583.331624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086807</td>\n",
       "      <td>-0.034171</td>\n",
       "      <td>0.026519</td>\n",
       "      <td>-0.042282</td>\n",
       "      <td>0.061826</td>\n",
       "      <td>0.079765</td>\n",
       "      <td>-0.035243</td>\n",
       "      <td>/storage/avatar2/2019-11-05_17-34-46/Cam0.mp4</td>\n",
       "      <td>10550</td>\n",
       "      <td>07:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 688 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "axes = data.emotions.plot()\n",
    "observation = data.aus\n",
    "labels = list(data.aus.columns)"
   ],
   "id": "8778558a7ead723"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ecb85654e1afb1ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обучение HMM на FACS Action Units",
   "id": "9aa515698d53f51f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_validate_fraction = .75   # train/validate\n",
    "min_state_n             = 2     # must be positive\n",
    "max_state_n             = 20    # must be grater than min_state_n\n",
    "attempts_to_fit_n      = 20    # number of attempts to fit a model\n",
    "                                # initial random state will different for each attempt\n",
    "show_progress_bar       = True  # show progress bar of overall training process\n",
    "print_log               = False # print a message per a trained model\n",
    "verbose                 = False # print verbose log of training for every model\n",
    "random_state            = 0     # random state seed, must be unsigned 32 bit integer\n",
    "n_samples_after         = 200   # number of states to generate after validating sequence\n",
    "raw_data_multiplier     = 100   # number to multiply data before conservation to integer \n",
    "\n",
    "X = np.array((raw_data_multiplier * (observation - np.min(observation))).astype(np.int_)) # for Poisson HMM all values must be positive\n",
    "# Sanity checks for PoissonHMM\n",
    "assert np.issubdtype(X.dtype, np.integer), \"X must be integer dtype\"\n",
    "assert X.min() >= 0, \"X must be non-negative\"\n",
    "\n",
    "train_validate_border = int(X.shape[0] * train_validate_fraction)\n",
    "\n",
    "X_train    = X[:train_validate_border]\n",
    "X_validate = X[train_validate_border:]\n",
    "\n",
    "scores = list()\n",
    "models = list()\n",
    "\n",
    "product = tqdm_itertools.product if show_progress_bar else itertools.product\n",
    "\n",
    "for idx, n_components in product(range(attempts_to_fit_n),\n",
    "                                     range(min_state_n, max_state_n)):\n",
    "        model = hmm.PoissonHMM(n_components = n_components,\n",
    "                               random_state=idx,\n",
    "                               verbose=verbose)\n",
    "        try:\n",
    "            model.fit(X_train)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(\"PoissonHMM fitting failed. Ensure observations are non-negative integers and sufficient in length.\") from e\n",
    "        models.append(model)\n",
    "        scores.append(model.score(X_validate))\n",
    "        if print_log:\n",
    "            print(f'Converged: {model.monitor_.converged}'\n",
    "                  f'\\tScore: {scores[-1]}')\n",
    "\n",
    "# get the best model\n",
    "model = models[np.argmax(scores)]\n",
    "print(f'The best model had a score of {max(scores)} and '\n",
    "      f'{model.n_components} components')\n",
    "\n",
    "# use the Viterbi algorithm to predict the most likely sequence of states\n",
    "# of the given the model for the train sequence\n",
    "states_train       = model.predict(X_train)\n",
    "# generate states with the model without any information from real data\n",
    "_, states_validate = model.sample(n_samples = len(X_validate) + n_samples_after,\n",
    "                              random_state = random_state)\n",
    "states = np.concatenate([states_train, states_validate])"
   ],
   "id": "4df3116b16bdf6d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Демонстрации реальных и сгенерированных данных на одном графике",
   "id": "134c5530540ce411"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot model states over time\n",
    "X_label         = list(map(lambda label: f\"real {label}\", labels))\n",
    "fitted_label     = list(map(lambda label: f\"predicted {label}\", labels))\n",
    "first_au_num    = 16\n",
    "last_au_num     = 100 # feel free to set value greater then the maximum AU number\n",
    "aus_range       = range(first_au_num, min(X.shape[1], model.lambdas_[states].shape[1], last_au_num))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in aus_range:\n",
    "    line, = ax.plot(X[:,i],\n",
    "            linestyle=\"--\",\n",
    "            label = X_label[i])\n",
    "    ax.plot(model.lambdas_[states][:,i],\n",
    "            color = line.get_color(),\n",
    "            label = fitted_label[i])\n",
    "ax.axvline(x = train_validate_border,\n",
    "           linestyle = 'dotted',\n",
    "           color = 'gray',\n",
    "           label = 'train/validate border')\n",
    "ax.axvline(x = X.shape[0],\n",
    "           linestyle = 'dashed',\n",
    "           color = 'gray',\n",
    "           label = 'end of real data')\n",
    "ax.legend(loc = (1.05,0))\n",
    "ax.set_title('Predicted data and real data comparation')\n",
    "ax.set_xlabel('Frame number')\n",
    "plt.show()"
   ],
   "id": "1e84da92332813ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Визуализация внутреннего устройства HMM\n",
    "Внутренние состояния HMM организованы в граф, на ребрах которого отмечены вероятности переходов. Изобразим его матрицу смежности и сам граф."
   ],
   "id": "e6e65de822c76340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ax.imshow(model.transmat_, aspect='auto', cmap='spring')\n",
    "plot = ax.pcolormesh(model.transmat_,\n",
    "               edgecolors='white',\n",
    "               linewidth=1)\n",
    "fig.colorbar(plot)\n",
    "ax.invert_yaxis()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Transition Matrix')\n",
    "ax.set_xlabel('State To')\n",
    "ax.set_ylabel('State From')\n",
    "plt.show()"
   ],
   "id": "9aedf3869a21c92a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "min_prob_to_plot_an_edge = 0.01\n",
    "float_format = \"{0:0.2f}\"\n",
    "\n",
    "matrix = np.array(model.transmat_)\n",
    "\n",
    "assert(len(matrix.shape) == 2)\n",
    "assert(matrix.shape[0] == matrix.shape[1])\n",
    "\n",
    "dot = graphviz.Digraph('HMM graph', comment='HMM graph')\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    dot.node(str(i), str(i))\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        prob = matrix[i, j]\n",
    "        if prob > min_prob_to_plot_an_edge:\n",
    "            formated_prob = float_format.format(prob)\n",
    "            dot.edge(str(i), str(j), label = float_format.format(prob))\n",
    "from IPython.display import display\n",
    "import shutil as _shutil\n",
    "if _shutil.which('dot'):\n",
    "    display(dot)\n",
    "else:\n",
    "    print(\"Graphviz 'dot' executable not found on PATH. Install system Graphviz to render diagrams.\")\n",
    "    print(\"Linux (Debian/Ubuntu): sudo apt-get install -y graphviz\")\n",
    "    print(\"macOS: brew install graphviz\")\n",
    "    print(\"Windows: Install Graphviz and add its bin directory to PATH\")\n",
    "    print(\"DOT source (you can paste this into an online Graphviz viewer):\")\n",
    "    print(dot.source)"
   ],
   "id": "f1806099ed7119ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Анимация аватара\n",
    "Ключевое в данном блокноте именно возможность обратного хода, то есть генерации поведения после обучения. HMM генерирует временной ряд FACS Action Units, по которым анимируется аватар. Для простоты интеграции с jupiter используется примитивная картинка из библиотеки `py-feat`, однако существуют готовые решения, которые позволяют по Action Units 3d модели, о них ниже.\n",
    "\n",
    "## Анимация по FACS Action Units\n",
    "### FACS Avatar\n",
    "Докер контейнер, позволяющий анимировать при помощи FACS action units модель в Blender, Unity или MakeHuman. [GitHub](https://github.com/NumesSanguis/FACSvatar), лицензия LGPL. Был собран на базе нескольких других готовых проектов одним человеком, сейчас не поддерживается."
   ],
   "id": "3d8ab985cef0467d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../why_FACS_control_trans.png')"
   ],
   "id": "6bcfea1026628dcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### OpenFace FACS Unity Facial Animator\n",
    "Докер контейнер, который позволяет при помощи Action Units анимировать аватар на Unity.\n",
    "[Github](https://github.com/alexismorin/OpenFace-FACS-Unity-Facial-Animator), лицензия LGPL. Входит в состав FACS Avatar.\n",
    "\n",
    "### OpenFacs\n",
    "Код на C++, которые локально рендерит аватар и позволят динамически менять его выражение лица посылая новые Action Units по UDP. Проект единожды написанный для статьи и с тех пор заброшен, но запускается и работает. [GitHub](https://github.com/phuselab/openFACS), лицензия MIT."
   ],
   "id": "5035cfb2da1f0664"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Анимация в этом блокноте\n",
    "Признаки выделялись только в каждом 25-ом кадре, то есть каждую секунду. Из-за этого движение глаз и рта хаотичны как на модели \"оригинале\", так и на аватаре."
   ],
   "id": "d040351bbadd847b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "from feat.plotting import plot_face\n",
    "\n",
    "def animate_au(fig, ax, aus, *args, **kwargs,):\n",
    "    camera = Camera(fig)\n",
    "    \n",
    "    for au in tqdm(aus):\n",
    "        # ax.clear()\n",
    "        ax = plot_face(\n",
    "            model=None,\n",
    "            ax=ax,\n",
    "            au=au,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        camera.snap()\n",
    "    ani = camera.animate()\n",
    "    return ani"
   ],
   "id": "59d35f7a59ec0127"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "animation_fps = 10\n",
    "animation_dpi = 300\n",
    "animation_filename = \"input.gif\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "ani = animate_au(fig, ax, X / float(raw_data_multiplier))\n",
    "print(\"Person's face\")\n",
    "HTML(ani.to_jshtml())"
   ],
   "id": "71211fcf1c9ffdd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "ani = animate_au(fig, ax, model.lambdas_[states] / float(raw_data_multiplier))\n",
    "print(\"Avatar's face\")\n",
    "HTML(ani.to_jshtml())"
   ],
   "id": "ded987b1dc67e966"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Эмоции как исходные данные для HMM\n",
    "В примере выше эмоции вообще никак не учитываются при моделировании. HMM принимает на вход только наблюдаемые Action Units, эмоций для нее не существует.\n",
    "\n",
    "Может оказаться, что лучше все таки концепцию эмоций использовать. Тогда пути данных в прямом и обратном направлении будут такими:\n",
    "```\n",
    "Image -> Land Marks -> FACS Action Units -> Emotions -> HMM\n",
    "```\n",
    "```\n",
    "HMM -> Emotions -> FACS Action Units -> Animator -> Image\n",
    "```\n",
    "## Распознавание эмоций\n",
    "Эмоции по FACS Action Units можно распознавать при помощи ML моделей, так и делает библиотека `py-feat`. Проблема в том, такое преобразование получается необратимым. По Action Units эмоции восстановить можно, а как из модели достать обратное преобразование не ясно.\n",
    "\n",
    "Будем использовать другой, более простой и не связанный с ML\n",
    "подход. Назовем эмоцией подпространство в пространстве action\n",
    "units. Выражение лица описывается вектором в Action Units пространстве. Степень выражения соответствия выражения лица эмоции равна углу между подпространством эмоции и вектором выражения лица.\n",
    "\n",
    "Некоторые эмоции могут иметь несколько способов выражения. Так удивляясь человек может приоткрыть рот, а может и нет. Правильнее сказать, что что эмоция -- класс эквивалентности подпространств в пространстве Action Units. Но будем для простоты считать, удивление с открытым ртом и закрытым -- разные эмоции."
   ],
   "id": "f47efb1e9ba14160"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Refactoring, Выделить связанные с эмоциями функции и структуры данных в класс.",
   "id": "408176d336efa260"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def auName2auNum(name : str):\n",
    "    return int(name[-2:])\n",
    "def auNum2auName(number : int):\n",
    "    return f\"AU{int(number):02d}\"\n",
    "def test_auName2auNum_auNum2auName():\n",
    "    for name in list(data.aus.columns):\n",
    "        number = auName2auNum(name)\n",
    "        name_test = auNum2auName(number)\n",
    "        if (name_test != name):\n",
    "            print(f\"Failed {name} -> {number} -> {name_test}\")\n",
    "            assert(False)\n",
    "test_auName2auNum_auNum2auName()"
   ],
   "id": "61c191c0bdc1541e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Данные об эмоциях-подпространствах возьмем из книги Paul Ekman, Wallace V. Friesen, Joseph C. Hager, \"Facial Action Coding System Investigator’s Guide\". В книге вводится интуитивно понятный DSL для их описания в виде формальных сумм. Звездочка (*) около номера Action Unit означает, что этот Action Unit может быть выражен в любой степени.",
   "id": "ed4ef5d2e4796748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# table from Facial Action Coding System Investigator’s Guide, by Paul Ekman, Wallace V. Friesen, Joseph C. Hager\n",
    "emotions = {\n",
    "    'anger' : {\n",
    "        '4+5*+7+10*+22+23+25',\n",
    "        '4+5*+7+23+25',\n",
    "        '4+5*+7+10*+22+23+26',\n",
    "        '4+5*+7+23+26',\n",
    "        '4+5*+7+17+23',\n",
    "        '4+5*+7+17+24',\n",
    "        '4+5*+7+23',\n",
    "        '4+5*+7+24',\n",
    "    },\n",
    "    'disgust' : {\n",
    "        '9',\n",
    "        '9+16+15',\n",
    "        '9+16+26',\n",
    "        '9+17',\n",
    "        '10*',\n",
    "        '10*+16+26',\n",
    "        '10*+16+25',\n",
    "        '10+17',\n",
    "    },\n",
    "    'fear' : {\n",
    "        '1+2+4+5*+20*+25',\n",
    "        '1+2+4+5*+25',\n",
    "        '1+2+4+5*+20*+26',\n",
    "        '1+2+4+5*+26',\n",
    "        '1+2+4+5*+20*+27',\n",
    "        '1+2+4+5*+27',\n",
    "    }, \n",
    "    'happiness' : {\n",
    "        '6+12*',\n",
    "        '12',\n",
    "    },\n",
    "    'sadness' : {\n",
    "        '1+4+11+15',\n",
    "        '1+4+15*',\n",
    "        '6+15*',\n",
    "    },\n",
    "    'surprise' : {\n",
    "        '1+2+5+26',\n",
    "        '1+2+5+27',\n",
    "    },\n",
    "    #'neutral' : {}\n",
    "}"
   ],
   "id": "50804fb900fd03e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подпространство эмоции будем описывать при помощи ортогонально проецирующей матрицы. `parseAdditiveCombination` конвертируют строку формальной суммы в матрицу-проектор на подпространство.",
   "id": "807fe4a9d852b459"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parseAdditiveCombination(combination: str, aus_space: list):\n",
    "    au_strs = combination.split('+')\n",
    "    N = len(aus_space)\n",
    "    if N == 0:\n",
    "        return None\n",
    "    # Build index for O(1) lookup\n",
    "    idx_by_au = {name: i for i, name in enumerate(aus_space)}\n",
    "\n",
    "    vector_sum = np.zeros(N)\n",
    "    direct_sum = []  # rows = basis vectors\n",
    "\n",
    "    for au_str in au_strs:\n",
    "        try:\n",
    "            add_to_direct_sum = (au_str[-1] == '*')\n",
    "            if add_to_direct_sum:\n",
    "                au_str = au_str[:-1]  # remove asterisk\n",
    "            au_name = auNum2auName(int(au_str))\n",
    "            idx = idx_by_au.get(au_name)\n",
    "            if idx is None:\n",
    "                # AU is not present in the current space; skip it\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid term \\\"{au_str}\\\" in additive combination \\\"{combination}\\\"\") from e\n",
    "\n",
    "        vector = np.zeros(N)\n",
    "        vector[idx] = 1\n",
    "\n",
    "        if add_to_direct_sum:\n",
    "            direct_sum.append(vector)\n",
    "        else:\n",
    "            vector_sum += vector\n",
    "\n",
    "    norm = np.linalg.norm(vector_sum)\n",
    "    if norm > 0:\n",
    "        vector_sum = vector_sum / norm\n",
    "        direct_sum.append(vector_sum)\n",
    "\n",
    "    if len(direct_sum) == 0:\n",
    "        return None\n",
    "    return np.stack(direct_sum, axis=0)\n",
    "\n",
    "def test_parseAdditiveCombination():\n",
    "    N = 3\n",
    "    au_labels = list(map(auNum2auName, np.arange(N) + 1))\n",
    "    ones = np.ones(N)\n",
    "    assert np.allclose(parseAdditiveCombination('1*+2*+3*', au_labels), np.eye(N))\n",
    "    expected = (ones / np.linalg.norm(ones))[None, :]\n",
    "    assert np.allclose(parseAdditiveCombination('1+2+3', au_labels), expected)\n",
    "    assert parseAdditiveCombination('42', au_labels) is None\n",
    "\n",
    "test_parseAdditiveCombination()"
   ],
   "id": "c19d8c0cced983fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Как уже было сказано выше, степень варажения соответствия выражения лица эмоции равнауглу между подпространством эмоции и ветором выражения лица.",
   "id": "5f98492d14bd365c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def subspace_angles_rows(A_rows: np.ndarray, B_rows: np.ndarray):\n",
    "    \"\"\"Compute principal angles between two subspaces with bases given in rows.\"\"\"\n",
    "    A = np.atleast_2d(A_rows)\n",
    "    B = np.atleast_2d(B_rows)\n",
    "    return scipy.linalg.subspace_angles(A.T, B.T)\n",
    "\n",
    "\n",
    "def emotion_value(emotion_projector: np.ndarray, face_expression: np.ndarray):\n",
    "    \"\"\"Angles between an emotion subspace (rows as basis) and a face-expression vector.\"\"\"\n",
    "    # If a vector is provided, make it a single row; if already a matrix, use as-is\n",
    "    if isinstance(face_expression, np.ndarray) and face_expression.ndim == 1:\n",
    "        v_rows = face_expression.reshape(1, -1)\n",
    "    else:\n",
    "        v_rows = np.atleast_2d(face_expression)\n",
    "    return subspace_angles_rows(emotion_projector, v_rows)"
   ],
   "id": "13a8fd2b6bb7b3db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "emotion_spaces = {}\n",
    "for emotion_name, sum_strs in emotions.items():\n",
    "    subspaces = []\n",
    "    for sum_str in sum_strs:\n",
    "        subspace = parseAdditiveCombination(sum_str, list(data.aus.columns))\n",
    "        if subspace is None:\n",
    "            continue\n",
    "        if len(subspaces) == 0:\n",
    "            subspaces.append(subspace)\n",
    "            continue\n",
    "        # check if subspace has already been added (near-zero principal angle)\n",
    "        angles_list = [subspace_angles_rows(subspace, existing) for existing in subspaces]\n",
    "        near_zero = any((angles_arr < 1e-8).any() for angles_arr in angles_list)\n",
    "        if not near_zero:\n",
    "            subspaces.append(subspace)\n",
    "    emotion_spaces[emotion_name] = subspaces\n",
    "\n",
    "# display(emotion_spaces)"
   ],
   "id": "2b331cb4bdc1765e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys, numpy, pandas, scipy, matplotlib, sklearn, hmmlearn\n",
    "import feat\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pandas:\", pandas.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"py-feat:\", getattr(feat, \"__version__\", \"n/a\"))\n",
    "print(\"hmmlearn:\", hmmlearn.__version__)"
   ],
   "id": "847bf3d1344e5e8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
