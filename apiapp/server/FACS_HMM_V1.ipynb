{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e28710-8873-4509-8c66-9eeb6d47f019",
   "metadata": {},
   "source": [
    "# Использование HMM на реальных данных\n",
    "\n",
    "Данный блокнот можно немного преувеличив минимальным прототипом аватара. Он демонстрирует выделение из видео признаков типа [F.A.C.S. Action Units]((https://en.wikipedia.org/wiki/Facial_Action_Coding_System#Method), обучение по ним скрытой марковской цепи (Hidden Markov Model, HMM) и анимацию предсказания.\n",
    "\n",
    "При анализе данных используется цепочка,\n",
    "```\n",
    "Image -> Land Marks -> FACS Action Units -> HMM\n",
    "```\n",
    "а в обратном ходе, при демонстрации аватаром лицевой анимации практически идентичная обратная\n",
    "```\n",
    "HMM -> FACS Action Units -> Animator -> Image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "9272f8dd-c24b-4cfa-b338-6cadf5c5f05c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import scipy\n",
    "import itertools\n",
    "from feat.data import Fex\n",
    "from feat.utils.io import read_feat\n",
    "\n",
    "from hmmlearn import hmm\n",
    "import tqdm\n",
    "import tqdm.contrib.itertools\n",
    "\n",
    "import graphviz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c67c07e1-d1a5-487b-805a-78b5e68737b9",
   "metadata": {},
   "source": [
    "## Выделение признаков\n",
    "\n",
    "Данные были получены при помощи библиотеки `py-feat`, которая умеет выделять из видео не только эмоции и опорные точки лица, но и Action Units системы FACS, что позволяет достаточно просто обратить преобразование и анимировать изображение."
   ]
  },
  {
   "cell_type": "code",
   "id": "aed6fdfc-4351-4749-b3e6-99a1d83b5eb3",
   "metadata": {},
   "source": [
    "from feat import Detector\n",
    "from feat.utils.io import get_test_data_path\n",
    "import os\n",
    "\n",
    "enabled     = False         # False means do nothing\n",
    "video_path  = '/fix/me'\n",
    "csv_path    = '/fix/me/too'\n",
    "fps         = 25\n",
    "skip_frames = fps           # start detection every second\n",
    "face_detection_threshold = 0.95\n",
    "\n",
    "if enabled:\n",
    "    detector = Detector()\n",
    "    \n",
    "    video_prediction = detector.detect_video(\n",
    "        video_path,\n",
    "        skip_frames=skip_frames,\n",
    "        face_detection_threshold=face_detection_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving to csv {csv_path}\")\n",
    "    video_prediction.to_csv(csv_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f58c835-b056-4a4e-a4a8-6024dc4f12ac",
   "metadata": {},
   "source": [
    "Класс `feat.data.Fex`, экземпляром которого является video_prediction является оберткой над pandas DataFrame. Данные из него можно сохранять стандартными методами `pandas`, прочитать данные из csv обратно в `feat.data.Fex` можно при помощи `feat.utils.io.read_feat`."
   ]
  },
  {
   "cell_type": "code",
   "id": "c8c21078-0865-42b7-856d-78522afa0a7e",
   "metadata": {},
   "source": [
    "data_path = '../data/2019-11-05_17-34-46-Cam0.csv'\n",
    "\n",
    "data = read_feat(data_path)\n",
    "display(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2cda6b86-40c0-4564-a3f4-ce58ded9e4df",
   "metadata": {},
   "source": [
    "axes = data.emotions.plot()\n",
    "observation = data.aus\n",
    "labels = list(data.aus.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9616bc4e-b7c9-408e-a0db-ed83ec9f4a62",
   "metadata": {},
   "source": [
    "## Обучение HMM на FACS Action Units"
   ]
  },
  {
   "cell_type": "code",
   "id": "777f9676-dd04-4917-afab-7b8a9df0653d",
   "metadata": {},
   "source": [
    "train_validate_fraction = .75   # train/validate\n",
    "min_state_n             = 2     # must be positive\n",
    "max_state_n             = 20    # must be grater than min_state_n\n",
    "attemptes_to_fit_n      = 20    # number of attempts to fit a model\n",
    "                                # initial random state will different for each attempt\n",
    "show_progress_bar       = True  # show progress bar of overall training process\n",
    "print_log               = False # print a message per a trained model\n",
    "verbose                 = False # print verbose log of training for every model\n",
    "random_state            = 0     # random state seed, must be unsigned 32 bit integer\n",
    "n_samples_after         = 200   # number of states to generate after validating sequence\n",
    "raw_data_multiplier     = 100   # number to multiply data before conservation to integer \n",
    "\n",
    "X = np.array((raw_data_multiplier * (observation - np.min(observation))).astype(np.int_)) # for Poisson HMM all values must be positive\n",
    "\n",
    "train_validate_border = int(X.shape[0] * train_validate_fraction)\n",
    "\n",
    "X_train    = X[:train_validate_border]\n",
    "X_validate = X[train_validate_border:]\n",
    "\n",
    "scores = list()\n",
    "models = list()\n",
    "\n",
    "product = tqdm.contrib.itertools.product if show_progress_bar else itertools.product\n",
    "\n",
    "try:\n",
    "    for idx, n_components in product(range(attemptes_to_fit_n),\n",
    "                                     range(min_state_n, max_state_n)):\n",
    "        model = hmm.PoissonHMM(n_components = n_components,\n",
    "                               random_state=idx,\n",
    "                               verbose=verbose)\n",
    "        \n",
    "        model.fit(X_train)\n",
    "        models.append(model)\n",
    "        scores.append(model.score(X_validate))\n",
    "        if print_log:\n",
    "            print(f'Converged: {model.monitor_.converged}'\n",
    "                  f'\\tScore: {scores[-1]}')\n",
    "except:\n",
    "    raise ValueError(\"PoissonHMM firring failed. Check if all values\"\n",
    "                     \"are positve integers and tere are enough values\"\n",
    "                     \"to fit hmm with souch amount of states.\")\n",
    "\n",
    "# get the best model\n",
    "model = models[np.argmax(scores)]\n",
    "print(f'The best model had a score of {max(scores)} and '\n",
    "      f'{model.n_components} components')\n",
    "\n",
    "# use the Viterbi algorithm to predict the most likely sequence of states\n",
    "# of the given the model for the train sequence\n",
    "states_train       = model.predict(X_train)\n",
    "# generate states with the model without any information from real data\n",
    "_, states_validate = model.sample(n_samples = len(X_validate) + n_samples_after,\n",
    "                              random_state = random_state,\n",
    "                              currstate = states_train[-1])\n",
    "states = np.concatenate([states_train, states_validate])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d10d0299-ab4b-44cf-b04d-c1090c1e2c4f",
   "metadata": {},
   "source": [
    "Демонстрации реальных и сгенерированных данных на одном графике"
   ]
  },
  {
   "cell_type": "code",
   "id": "18adcaec-cb5a-462c-be70-14020bb7ea1f",
   "metadata": {},
   "source": [
    "# plot model states over time\n",
    "X_label         = list(map(lambda label: f\"real {label}\", labels))\n",
    "fited_label     = list(map(lambda label: f\"predicted {label}\", labels))\n",
    "first_au_num    = 16\n",
    "last_au_num     = 100 # feel free to set value greater then the maximum AU number\n",
    "aus_range       = range(first_au_num, min(X.shape[1], model.lambdas_[states].shape[1], last_au_num))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in aus_range:\n",
    "    line, = ax.plot(X[:,i],\n",
    "            linestyle=\"--\",\n",
    "            label = X_label[i])\n",
    "    ax.plot(model.lambdas_[states][:,i],\n",
    "            color = line.get_color(),\n",
    "            label = fited_label[i])\n",
    "ax.axvline(x = train_validate_border,\n",
    "           linestyle = 'dotted',\n",
    "           color = 'gray',\n",
    "           label = 'tarain/validate border')\n",
    "ax.axvline(x = X.shape[0],\n",
    "           linestyle = 'dashed',\n",
    "           color = 'gray',\n",
    "           label = 'end of real data')\n",
    "ax.legend(loc = (1.05,0))\n",
    "ax.set_title('Predicted data and real data comparation')\n",
    "ax.set_xlabel('Frame number')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "722a9fbe-5dda-44f9-8e80-528fc8fca4fb",
   "metadata": {},
   "source": [
    "## Визуализация внутреннего устройства HMM\n",
    "Внутренние состояния HMM организованы в граф, на ребрах которого отмечены вероятности переходов. Изобразим его матрицу смежности и сам граф."
   ]
  },
  {
   "cell_type": "code",
   "id": "5a96f1bd-034d-4697-a009-d514b0783f8a",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ax.imshow(model.transmat_, aspect='auto', cmap='spring')\n",
    "plot = ax.pcolormesh(model.transmat_,\n",
    "               edgecolors='white',\n",
    "               linewidth=1)\n",
    "fig.colorbar(plot)\n",
    "ax.invert_yaxis()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Transition Matrix')\n",
    "ax.set_xlabel('State To')\n",
    "ax.set_ylabel('State From')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ee0e700-fcda-4ff3-9efc-ff6634587171",
   "metadata": {},
   "source": [
    "min_prob_to_plot_an_edge = 0.01\n",
    "float_format = \"{0:0.2f}\"\n",
    "\n",
    "matrix = np.array(model.transmat_)\n",
    "\n",
    "assert(len(matrix.shape) == 2)\n",
    "assert(matrix.shape[0] == matrix.shape[0])\n",
    "\n",
    "dot = graphviz.Digraph('HMM graph', comment='HMM graph')\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    dot.node(str(i), str(i))\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        prob = matrix[i, j]\n",
    "        if prob > min_prob_to_plot_an_edge:\n",
    "            formated_prob = float_format.format(prob)\n",
    "            dot.edge(str(i), str(j), label = float_format.format(prob))\n",
    "dot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "486b3390-a8e5-43dc-8ffb-11365783af4b",
   "metadata": {},
   "source": [
    "# Анимация аватара\n",
    "Ключевое в данном блокноте именно возможность обратного хода, то есть генерации поведения после обучения. HMM генерирует временной ряд FACS Action Units, по которым анимируется аватар. Для простоты интеграции с jupiter используется примитивная картинка из библиотеки `py-feat`, однако существуют готовые решения, которые позволяют по Action Units 3d модели, о них ниже.\n",
    "\n",
    "## Анимация по FACS Action Units\n",
    "### FACS Avatar\n",
    "Докер контейнер, позволяющий анимировать при помощи FACS action units модель в Blender, Unity или MakeHuman. [GitHub](https://github.com/NumesSanguis/FACSvatar), лицензия LGPL. Был собран на базе нескольких других готовых проектов одним человеком, сейчас не поддерживается."
   ]
  },
  {
   "cell_type": "code",
   "id": "a93b1757-4a56-469d-ad7b-130e7c8e972b",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../why_FACS_control_trans.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d602f67-a5ea-4732-bf8c-508e3cab6579",
   "metadata": {},
   "source": [
    "### OpenFace FACS Unity Facial Animator\n",
    "Докер контейнер, который позволяет при помощи Action Units анимировать аватар на Unity.\n",
    "[Github](https://github.com/alexismorin/OpenFace-FACS-Unity-Facial-Animator), лицензия LGPL. Входит в состав FACS Avatar.\n",
    "\n",
    "### OpenFacs\n",
    "Код на C++, которые локально рендерит аватар и позволят динамически менять его выражение лица посылая новые Action Units по UDP. Проект единожды написанный для статьи и с тех пор заброшен, но запускается и работает. [GitHub](https://github.com/phuselab/openFACS), лицензия MIT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38fd93-0cc8-445d-b3a3-426845815d3f",
   "metadata": {},
   "source": [
    "## Анимация в этом блокноте\n",
    "Признаки выделялись только в каждом 25-ом кадре, то есть каждую секунду. Из-за этого движение глаз и рта хаотичны как на модели \"оригинале\", так и на аватаре."
   ]
  },
  {
   "cell_type": "code",
   "id": "56351bfe-b8be-4fe5-b6ac-d45aa01c30a5",
   "metadata": {},
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "from feat.plotting import plot_face\n",
    "\n",
    "def animate_au(fig, ax, aus, *args, **kwargs,):\n",
    "    camera = Camera(fig)\n",
    "    \n",
    "    for au in tqdm.tqdm(aus):\n",
    "        # ax.clear()\n",
    "        ax = plot_face(\n",
    "            model=None,\n",
    "            ax=ax,\n",
    "            au=au,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        camera.snap()\n",
    "    ani = camera.animate()\n",
    "    return ani"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9933090-d079-415b-9e77-ff169aa6b38c",
   "metadata": {},
   "source": [
    "animation_fps = 10\n",
    "animation_dpi = 300\n",
    "animation_filename = \"input.gif\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "ani = animate_au(fig, ax, X / float(raw_data_multiplier))\n",
    "print(\"Person's face\")\n",
    "HTML(ani.to_jshtml())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29254d29-9fc4-4e09-b5f4-08e11c97d638",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "ani = animate_au(fig, ax, model.lambdas_[states] / float(raw_data_multiplier))\n",
    "print(\"Avatar's face\")\n",
    "HTML(ani.to_jshtml())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "106eb7c0-35f7-4ae9-9e2b-793b96a13798",
   "metadata": {},
   "source": [
    "# Эмоции как исходные данные для HMM\n",
    "В примере выше эмоции вообще никак не учитываются при моделировании. HMM принимает на вход только наблюдаемые Action Units, эмоций для нее не существует.\n",
    "\n",
    "Может оказаться, что лучше все таки концепцию эмоций использовать. Тогда пути данных в прямом и обратном направлении будут такими:\n",
    "```\n",
    "Image -> Land Marks -> FACS Action Units -> Emotions -> HMM\n",
    "```\n",
    "```\n",
    "HMM -> Emotions -> FACS Action Units -> Animator -> Image\n",
    "```\n",
    "## Распознавание эмоций\n",
    "Эмоции по FACS Action Units можно распознавать при помощи ML моделей, так и делает библиотека `py-feat`. Проблема в том, такое преобразование получается необратимым. По Action Units эмоции восстановить можно, а как из модели достать обратное преобразование не ясно.\n",
    "\n",
    "Будем использовать другой, более простой и не связанный с ML\n",
    "подход. Назовем эмоцией подпространство в пространстве action\n",
    "units. Выражение лица описывается вектором в Action Units пространстве. Степень выражения соответствия выражения лица эмоции равна углу между подпространством эмоции и вектором выражения лица.\n",
    "\n",
    "Некоторые эмоции могут иметь несколько способов выражения. Так удивляясь человек может приоткрыть рот, а может и нет. Правильнее сказать, что что эмоция -- класс эквивалентности подпространств в пространстве Action Units. Но будем для простоты считать, удивление с открытым ртом и закрытым -- разные эмоции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc94a3-0de0-4f5b-93a2-529552963d45",
   "metadata": {},
   "source": [
    "TODO: Refactoring, Выделить связанные с эмоциями функции и структуры данных в класс."
   ]
  },
  {
   "cell_type": "code",
   "id": "04944084-4939-45e1-b625-a819a01599ae",
   "metadata": {},
   "source": [
    "def auName2auNum(name : str):\n",
    "    return int(name[-2:])\n",
    "def auNum2auName(number : int):\n",
    "    return f\"AU{int(number):02d}\"\n",
    "def test_auName2auNum_auNum2auName():\n",
    "    for name in list(data.aus.columns):\n",
    "        number = auName2auNum(name)\n",
    "        name_test = auNum2auName(number)\n",
    "        if (name_test != name_test):\n",
    "            print(f\"Failed {name} -> {number} -> {name_test}\")\n",
    "            assert(False)\n",
    "test_auName2auNum_auNum2auName()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf5f89dd-f731-457d-88b6-f292f1726b0a",
   "metadata": {},
   "source": [
    "Данные об эмоциях-подпространствах возьмем из книги Paul Ekman, Wallace V. Friesen, Joseph C. Hager, \"Facial Action Coding System Investigator’s Guide\". В книге вводится интуитивно понятный DSL для их описания в виде формальных сумм. Звездочка (*) около номера Action Unit означает, что этот Action Unit может быть выражен в любой степени."
   ]
  },
  {
   "cell_type": "code",
   "id": "493e5927-12ed-496f-9c88-7e62ac5a7b81",
   "metadata": {},
   "source": [
    "# table from Facial Action Coding System Investigator’s Guide, by Paul Ekman, Wallace V. Friesen, Joseph C. Hager\n",
    "emotions = {\n",
    "    'anger' : {\n",
    "        '4+5*+7+10*+22+23+25',\n",
    "        '4+5*+7+23+25',\n",
    "        '4+5*+7+10*+22+23+26',\n",
    "        '4+5*+7+23+26',\n",
    "        '4+5*+7+17+23',\n",
    "        '4+5*+7+17+24',\n",
    "        '4+5*+7+23',\n",
    "        '4+5*+7+24',\n",
    "    },\n",
    "    'disgust' : {\n",
    "        '9',\n",
    "        '9+16+15',\n",
    "        '9+16+26',\n",
    "        '9+17',\n",
    "        '10*',\n",
    "        '10*+16+26',\n",
    "        '10*+16+25',\n",
    "        '10+17',\n",
    "    },\n",
    "    'fear' : {\n",
    "        '1+2+4+5*+20*+25',\n",
    "        '1+2+4+5*+25',\n",
    "        '1+2+4+5*+20*+26',\n",
    "        '1+2+4+5*+26',\n",
    "        '1+2+4+5*+20*+27',\n",
    "        '1+2+4+5*+27',\n",
    "    }, \n",
    "    'happiness' : {\n",
    "        '6+12*',\n",
    "        '12',\n",
    "    },\n",
    "    'sadness' : {\n",
    "        '1+4+11+15',\n",
    "        '1+4+15*',\n",
    "        '6+15*',\n",
    "    },\n",
    "    'surprise' : {\n",
    "        '1+2+5+26',\n",
    "        '1+2+5+27',\n",
    "    },\n",
    "    #'neutral' : {}\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "821ce674-d03e-485d-b52e-84b34e3e84bb",
   "metadata": {},
   "source": [
    "Подпространство эмоции будем описывать при помощи ортогонально проецирующей матрицы. `parseAdditiveCombination` конвертируют строку формальной суммы в матрицу-проектор на подпространство."
   ]
  },
  {
   "cell_type": "code",
   "id": "a6c0407f-0d7d-41c9-81c2-9320cc30b27c",
   "metadata": {},
   "source": [
    "def parseAdditiveCombination(combination : str, aus_space : list):\n",
    "    au_strs    = combination.split('+')\n",
    "    N          = len(aus_space)\n",
    "    vector_sum = np.zeros(N)\n",
    "    direct_sum = []\n",
    "    for au_str in au_strs:\n",
    "        try:\n",
    "            add_to_direct_summ = (au_str[-1] == '*')\n",
    "            if add_to_direct_summ:\n",
    "                au_str = au_str[:-1] # remove asterisk\n",
    "            if aus_space.count(auNum2auName(au_str)) <= 0:\n",
    "                continue\n",
    "        except:\n",
    "            raise ValueError(f\"Invelid term \\\"{au_str}\\\" in additive combination \\\"{combination}\\\"\")\n",
    "\n",
    "        vector = np.zeros(N)\n",
    "        vector[aus_space.index(auNum2auName(au_str))] = 1\n",
    "        \n",
    "        if add_to_direct_summ:\n",
    "            direct_sum.append(vector)\n",
    "        else:\n",
    "            vector_sum = vector_sum + vector\n",
    "    vector_sum_norm = np.linalg.norm(vector_sum)\n",
    "    if vector_sum_norm != 0:\n",
    "        vector_sum = vector_sum / vector_sum_norm\n",
    "        direct_sum.append(vector_sum)\n",
    "    # TODO check that row of matrix to return is orthogonal\n",
    "    return None if len(direct_sum) == 0 else np.stack(direct_sum)\n",
    "\n",
    "def test_parseAdditiveCombination():\n",
    "    N = 3\n",
    "    au_labes = list(map(auNum2auName, np.arange(N) + 1)) # ['AU01', 'AU2', ...]\n",
    "    ones = np.repeat(1, N)\n",
    "    assert (parseAdditiveCombination('1*+2*+3*', au_labes) == np.identity(N)).all()\n",
    "    assert (parseAdditiveCombination('1+2+3', au_labes) == ones / np.linalg.norm(ones)).all()\n",
    "    assert parseAdditiveCombination('42', au_labes) is None\n",
    "\n",
    "test_parseAdditiveCombination()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f84e583a-bec0-42e0-b8f7-2740d23d6bbd",
   "metadata": {},
   "source": [
    "Как уже было сказано выше, степень варажения соответствия выражения лица эмоции равнауглу между подпространством эмоции и ветором выражения лица."
   ]
  },
  {
   "cell_type": "code",
   "id": "db9e13be-63d0-49ad-b496-f8ac1233e4c6",
   "metadata": {},
   "source": [
    "def emotion_value(emotion_projector: np.ndarray, face_expression : np.ndarray):\n",
    "    return scipy.linalg.subspace_angles(emotion_projector.T, face_expression.T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "857746ae-2d9e-4739-a7bc-5e7d2f845c09",
   "metadata": {},
   "source": [
    "emotion_spaces = {}\n",
    "for emotion_name, sum_strs in emotions.items():\n",
    "    subspaces = []\n",
    "    for sum_str in sum_strs:\n",
    "        subspace = parseAdditiveCombination(sum_str, list(data.aus.columns))\n",
    "        if len(subspaces) == 0:\n",
    "            subspaces.append(subspace)\n",
    "            continue\n",
    "        # check if subspace have already been added\n",
    "        angles = map(emotion_value, itertools.repeat(subspace), subspaces)\n",
    "        there_is_a_zero_angle = sum(list(map(lambda angles_arr: (angles_arr == 0).any(), angles)))\n",
    "        if there_is_a_zero_angle:\n",
    "            subspaces.append(subspace)\n",
    "    emotion_spaces[emotion_name] = subspaces\n",
    "\n",
    "# display(emotion_spaces)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
